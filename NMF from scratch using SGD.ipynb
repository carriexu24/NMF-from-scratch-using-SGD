{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\n",
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "vocab = np.array(vectorizer_tfidf.get_feature_names())\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "d = 5 # num topics\n",
    "clf = decomposition.NMF(n_components=d, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people don think just like objective say morality',\n",
       " 'graphics thanks files image file program windows know',\n",
       " 'space nasa launch shuttle orbit moon lunar earth',\n",
       " 'ico bobbe tek beauchaine bronx manhattan sank queens',\n",
       " 'god jesus bible believe christian atheism does belief']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF from scratch using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stochastic gradient descent (SGD), we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is why it is stochastic. It turns out that this is still an effective way to optimize, and it's much more efficient!\n",
    "\n",
    "\n",
    "Applying SGD to NMF\n",
    "\n",
    "Goal: Decompose $V\\;(m \\times n)$ into $$V \\approx WH$$ where $W\\;(m \\times d)$ and $H\\;(d \\times n)$, $W,\\;H\\; \\geq \\;0$, and we've minimized the Frobenius norm of $V-WH$.\n",
    "\n",
    "Approach: We will pick random positive $W$ & $H$, and then use SGD to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grads(M, W, H):\n",
    "    R = W@H-M\n",
    "    return R@H.T + penalty(W, mu)*lam, W.T@R + penalty(H, mu)*lam # dW, dH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penalty(M, mu):\n",
    "    return np.where(M>=mu, 0, np.min(M - mu, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upd(M, W, H, lr):\n",
    "    dW,dH = grads(M,W,H)\n",
    "    W -= lr*dW; H -= lr*dH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(M,W,H): \n",
    "    print(np.linalg.norm(M-W@H), W.min(), H.min(), (W<0).sum(), (H<0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = vectors_tfidf.shape\n",
    "\n",
    "W = np.abs(np.random.normal(scale=0.01, size=(m,d)))\n",
    "H = np.abs(np.random.normal(scale=0.01, size=(d,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = 1e-6\n",
    "lam = 1e3\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.4179745263 -0.000848008936649 -7.1156988209e-05 137 306\n",
      "44.3774945487 -0.000327947903727 -5.85181157935e-05 46 450\n",
      "44.3487344257 -0.000235325707557 -8.46750926792e-05 33 925\n",
      "44.317056816 -0.000169573014055 -8.40447187289e-05 36 1422\n",
      "44.2820416704 -0.000105532968983 -0.000102598904842 38 2117\n",
      "CPU times: user 1min 45s, sys: 19.4 s, total: 2min 4s\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(50): \n",
    "    upd(vectors_tfidf,W,H,lr)\n",
    "    if i % 10 == 0: report(vectors_tfidf,W,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don god space know think people just like',\n",
       " 'god space just people like don think does',\n",
       " 'space people don like think does god just',\n",
       " 'god don know space people just think does',\n",
       " 'god don space people think just time like']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is very slow to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grads_t(M, W, H):\n",
    "    R = W.mm(H)-M\n",
    "    return (R.mm(H.t()) + penalty_t(W, mu)*lam, \n",
    "        W.t().mm(R) + penalty_t(H, mu)*lam) # dW, dH\n",
    "\n",
    "def penalty_t(M, mu):\n",
    "    return (M<mu).type(torch.FloatTensor)*torch.clamp(M - mu, max=0.)\n",
    "\n",
    "def upd_t(M, W, H, lr):\n",
    "    dW,dH = grads_t(M,W,H)\n",
    "    W.sub_(lr*dW); H.sub_(lr*dH)\n",
    "\n",
    "def report_t(M,W,H): \n",
    "    print((M-W.mm(H)).norm(2), W.min(), H.min(), (W<0).sum(), (H<0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vectors = torch.Tensor(vectors_tfidf.astype(np.float32))\n",
    "\n",
    "t_W = torch.FloatTensor(m,d).normal_(std=0.01).abs_()\n",
    "t_H = torch.FloatTensor(d,n).normal_(std=0.01).abs_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.41695271374714 -0.0009815343655645847 -7.52935666241683e-05 138 250\n",
      "44.380284272501655 -0.0004332782991696149 -4.4321739551378414e-05 127 432\n",
      "44.35766005798296 -0.00019702206191141158 -5.1875977078452706e-05 123 872\n",
      "44.33610793274646 -0.00014351926802191883 -5.264638821245171e-05 112 1368\n",
      "44.31470034494246 -0.0001014827357721515 -6.394876254489645e-05 131 2019\n",
      "CPU times: user 54.8 s, sys: 8.11 s, total: 1min 2s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(50): \n",
    "    upd_t(t_vectors,t_W,t_H,lr)\n",
    "    if i % 10 == 0: \n",
    "        report_t(t_vectors,t_W,t_H)\n",
    "        lr *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['god space people know think just don like',\n",
       " 'space god don think people just like know',\n",
       " 'don space people just like god does know',\n",
       " 'don just people space think god know time',\n",
       " 'space don god know like people just does']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(t_H.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using PyTorch and Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penalty(A):\n",
    "    return torch.pow((A<0).type(torch.FloatTensor)*torch.clamp(A, max=0.), 2)\n",
    "\n",
    "def penalize(): \n",
    "    return penalty(pW).mean() + penalty(pH).mean()\n",
    "\n",
    "def loss(): \n",
    "    return (M-pW.mm(pH)).norm(2) + penalize()*lam\n",
    "\n",
    "def report():\n",
    "    W,H = pW.data, pH.data\n",
    "    print((M-pW.mm(pH)).norm(2).data[0], W.min(), H.min(), (W<0).sum(), (H<0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Variable(t_vectors)\n",
    "\n",
    "pW = Variable(torch.FloatTensor(m,d), requires_grad=True)\n",
    "pH = Variable(torch.FloatTensor(d,n), requires_grad=True)\n",
    "pW.data.normal_(std=0.01).abs_()\n",
    "pH.data.normal_(std=0.01).abs_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam([pW,pH], lr=1e-3, betas=(0.9,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.40464782714844 -0.0009986847871914506 -0.0009995140135288239 499 8492\n",
      "44.29405975341797 -0.0060671851970255375 -0.010416567325592041 659 54102\n",
      "44.22111892700195 -0.007334951777011156 -0.01876099593937397 499 54590\n",
      "44.1679801940918 -0.00876869261264801 -0.024152137339115143 1068 60384\n",
      "44.13126754760742 -0.01158764399588108 -0.031652577221393585 1525 59600\n",
      "CPU times: user 1min 45s, sys: 19.7 s, total: 2min 5s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(50): \n",
    "    opt.zero_grad()\n",
    "    l = loss()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "    if i % 10 == 0: \n",
    "        report()\n",
    "        lr *= 0.9     # learning rate annealling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['way post example know using thank things exist',\n",
       " 'religion make software god years time didn real',\n",
       " 'moon morality data louis years like called idea',\n",
       " 'christian color povray file rtrace post want help',\n",
       " 'sure actually line using atheism mac bible religion']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(pH.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
